import streamlit as st
import pandas as pd
import os
import base64
import time
import requests
from bs4 import BeautifulSoup
import urllib3

from utils.cleaning import clean_data
from utils.dashboard import display_dashboard
from utils.scraping import scrap_data

# D√©sactiver les warnings SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

st.set_page_config(page_title="My Scraping App", page_icon="üöÄ", layout="wide")

st.sidebar.title("Menu")

URLS = {
    "chiens": "https://sn.coinafrique.com/categorie/chiens?page={}",
    "moutons": "https://sn.coinafrique.com/categorie/moutons?page={}",
    "poules": "https://sn.coinafrique.com/categorie/poules-lapins-et-pigeons?page={}",
    "autres": "https://sn.coinafrique.com/categorie/autres-animaux?page={}"
}
option = st.sidebar.radio(
    "Choisir une option :",
    ("Scraping", "Dashboard", "WebScraper Data", "√âvaluer l'App")
)

# üìå Scraping
# def scrap_data(base_url, category, max_pages=10):
#     all_data = []
#     headers = {
#         "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) "
#                       "Chrome/132.0.0.0 Safari/537.36"
#     }
#
#     for page_num in range(1, max_pages + 1):
#         url = base_url.format(page_num)
#         print(f"üîç Scraping de la page {page_num} : {url}")
#
#         # V√©rifier si l'URL est valide avant de scraper
#         response = requests.head(url, headers=headers, verify=False)
#         if response.status_code == 404:
#             print(f"‚ö†Ô∏è La page {page_num} n'existe pas (HTTP 404). Arr√™t du scraping.")
#             break  # Arr√™ter le scraping si la page n'existe pas
#
#         response = requests.get(url, headers=headers, verify=False)
#         if response.status_code != 200:
#             print(f"‚ö†Ô∏è Erreur sur la page {page_num} : HTTP {response.status_code}")
#             continue
#
#         soup = BeautifulSoup(response.text, "html.parser")
#
#         containers = soup.select(".item-inner.mv-effect-translate-1.mv-box-shadow-gray-1")
#         print(f"üîé Trouv√© {len(containers)} annonces sur la page {page_num}")
#
#         for container in containers:
#             try:
#                 details_elem = container.find(class_="content-desc")
#                 details = details_elem.get_text(strip=True) if details_elem else "N/A"
#
#                 prices_elements = container.find_all(class_="content-price")
#                 if prices_elements:
#                     price_text = prices_elements[0].get_text(strip=True).replace("FCFA", "").replace(",", "").strip()
#                     location_text = prices_elements[1].get_text(strip=True) if len(prices_elements) > 1 else "N/A"
#                 else:
#                     price_text = "N/A"
#                     location_text = "N/A"
#
#                 image_element = container.select_one("h2 a img")
#                 image_url = image_element["src"].strip() if image_element and image_element.has_attr("src") else "N/A"
#
#                 all_data.append({
#                     "d√©tails": details,
#                     "prix": price_text,
#                     "localisation": location_text,
#                     "image URL": image_url
#                 })
#             except Exception as e:
#                 print(f"‚ö†Ô∏è Erreur lors de l'extraction d'une annonce : {e}")
#
#         # D√©lai pour √©viter d‚Äô√™tre bloqu√© par le site
#         time.sleep(3)
#
#     # üìå V√©rification des donn√©es existantes
#     file_path = f"data/{category.lower().replace(' ', '_')}.csv"
#     if os.path.exists(file_path):
#         print(f"üìÇ Chargement des anciennes donn√©es depuis : {file_path}")
#         old_df = pd.read_csv(file_path)
#     else:
#         old_df = pd.DataFrame()
#
#     # üìå Fusion des nouvelles et anciennes donn√©es uniquement si du nouveau contenu a √©t√© scrap√©
#     if all_data:
#         new_df = pd.DataFrame(all_data)
#         combined_df = pd.concat([old_df, new_df], ignore_index=True).drop_duplicates()
#         combined_df.to_csv(file_path, index=False)
#         print(f"‚úÖ Donn√©es sauvegard√©es pour {category} dans : {file_path}")
#     else:
#         print(f"‚ö†Ô∏è Aucun nouveau r√©sultat. Conservation des anciennes donn√©es.")

# üìå Fonction pour charger les anciennes donn√©es
def load_existing_data():
    if selected_category == "Toutes les cat√©gories":
        for category in URLS.keys():
            file_path = f"data/selenium_data/{category.lower().replace(' ', '_')}.csv"
            try:
                df = pd.read_csv(file_path)
                if clean_data_option:
                    df = clean_data(df)
                st.markdown(f"### üìä Donn√©es existantes pour **{category}**")
                st.dataframe(df)
            except FileNotFoundError:
                st.warning(f"‚ö†Ô∏è Aucune donn√©e existante pour {category}.")
    else:
        file_path = f"data/selenium_data/{selected_category.lower().replace(' ', '_')}.csv"
        try:
            df = pd.read_csv(file_path)
            if clean_data_option:
                df = clean_data(df)
            st.markdown(f"### üìä Donn√©es existantes pour **{selected_category}**")
            st.dataframe(df)
        except FileNotFoundError:
            st.warning(f"‚ö†Ô∏è Aucune donn√©e existante pour {selected_category}.")

# üìå Gestion du Scraping
if option == "Scraping":
    st.header("Scraping de donn√©es")
    st.write("Scrapez des donn√©es √† partir de plusieurs pages.")

    selected_category = st.selectbox("Choisir une cat√©gorie :", ["Toutes les cat√©gories"] + list(URLS.keys()))
    num_pages = st.number_input("Nombre de pages √† scraper :", min_value=1, max_value=50, value=2)
    clean_data_option = st.checkbox("Nettoyer les donn√©es avant affichage", value=True)

    load_existing_data()

    if st.button("Lancer le scraping"):
        with st.spinner("Scraping en cours..."):
            if selected_category == "Toutes les cat√©gories":
                for category, url in URLS.items():
                    scrap_data(url, category, max_pages=num_pages)
            else:
                scrap_data(URLS[selected_category], selected_category, max_pages=num_pages)
            st.rerun()

# üìå Gestion du Dashboard
elif option == "Dashboard":
    st.header("üìà Dashboard des donn√©es scrap√©es")
    selected_dashboard_category = st.selectbox("Choisir une cat√©gorie √† analyser :", list(URLS.keys()))
    file_path = f"data/webscraper_data/{selected_dashboard_category.lower().replace(' ', '_')}.csv"
    try:
        df_dashboard = pd.read_csv(file_path)
        display_dashboard(df_dashboard, selected_dashboard_category)
    except FileNotFoundError:
        st.warning(f"‚ö†Ô∏è Aucune donn√©e trouv√©e pour {selected_dashboard_category}. Veuillez scraper d'abord.")

# üìå Option pour t√©l√©charger les donn√©es
elif option == "WebScraper Data":
    st.header("üåê Donn√©es WebScraper")
    data_folder = "data/webscraper_data"
    if not os.path.exists(data_folder):
        st.warning("‚ö†Ô∏è Aucune donn√©e trouv√©e. Scrapez d'abord !")
    else:
        for file in os.listdir(data_folder):
            file_path = os.path.join(data_folder, file)
            with open(file_path, "rb") as f:
                st.download_button(f"‚¨áÔ∏è T√©l√©charger {file}", data=f, file_name=file)

# üìå √âvaluation de l'application
elif option == "√âvaluer l'App":
    st.header("√âvaluer l'application")
    st.write("Merci de remplir ce formulaire pour nous aider √† am√©liorer l'application.")
    kobo_form_url = "https://ee.kobotoolbox.org/x/bCaC927U"  # Remplace par ton propre lien
    iframe_code = f'<iframe src="{kobo_form_url}" width="100%" height="600px"></iframe>'
    st.markdown(iframe_code, unsafe_allow_html=True)

