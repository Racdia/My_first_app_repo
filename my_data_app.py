

import streamlit as st
import pandas as pd
import os
import  matplotlib as plt
import seaborn as sns

from utils.cleaning import clean_data
from utils.dashboard import display_dashboard
from utils.scraping import scrap_data

# Configuration de l'application
st.set_page_config(page_title="My Scraping App", page_icon="üöÄ", layout="wide")

# Titre de l'application
st.title("üöÄ My Scraping App")
URLS = {
    "V√©hicules": "https://dakarvente.com/index.php?page=annonces_rubrique&url_categorie_2=vehicules&id=2&sort=&nb={}",
    "Motos": "https://dakarvente.com/index.php?page=annonces_categorie&id=3&sort=&nb={}",
    "Voitures en location": "https://dakarvente.com/index.php?page=annonces_categorie&id=8&sort=&nb={}",
    "T√©l√©phones": "https://dakarvente.com/index.php?page=annonces_categorie&id=32&sort=&nb={}"
}
# Sidebar (Menu lat√©ral)
st.sidebar.title("Menu")
option = st.sidebar.radio(
    "Choisir une option :",
    ("Scraping", "Dashboard", "WebScraper Data", "√âvaluer l'App")
)

# Option 1 : Scraping
if option == "Scraping":
    st.header("Scraping de donn√©es")
    st.write("Scrapez des donn√©es √† partir de plusieurs pages.")

    # S√©lection de la cat√©gorie et du nombre de pages
    selected_category = st.selectbox("Choisir une cat√©gorie :", ["Toutes les cat√©gories"] + list(URLS.keys()))
    num_pages = st.number_input("Nombre de pages √† scraper :", min_value=1, max_value=50, value=2)
    clean_data_option = st.checkbox("Nettoyer les donn√©es avant affichage", value=True)

    # Afficher les donn√©es existantes par d√©faut
    def load_existing_data():
        """Charger et afficher les donn√©es existantes dans les fichiers CSV."""
        if selected_category == "Toutes les cat√©gories":
            for category in URLS.keys():
                file_path = f"data/{category.lower().replace(' ', '_')}.csv"
                try:
                    df = pd.read_csv(file_path)
                    if clean_data_option:
                        df = clean_data(df)
                    st.markdown(f"### üìä Donn√©es existantes pour **{category}**")
                    st.dataframe(df)
                except FileNotFoundError:
                    st.warning(f"‚ö†Ô∏è Aucune donn√©e existante pour {category}.")
        else:
            file_path = f"data/{selected_category.lower().replace(' ', '_')}.csv"
            try:
                df = pd.read_csv(file_path)
                if clean_data_option:
                    df = clean_data(df)
                st.markdown(f"### üìä Donn√©es existantes pour **{selected_category}**")
                st.dataframe(df)
            except FileNotFoundError:
                st.warning(f"‚ö†Ô∏è Aucune donn√©e existante pour {selected_category}.")

    # Charger les donn√©es existantes d√®s l'ouverture de l'onglet
    load_existing_data()

    # Bouton pour lancer le scraping
    if st.button("Lancer le scraping"):
        with st.spinner("Scraping en cours..."):
            if selected_category == "Toutes les cat√©gories":
                all_data = {}
                for category, url in URLS.items():
                    st.markdown(f"### üîÑ Scraping des **{category}**...")
                    data = scrap_data(url, max_pages=num_pages)
                    df = pd.DataFrame(data)
                    if clean_data_option:
                        df = clean_data(df)
                    all_data[category] = df
                    st.markdown(f"### üìä Donn√©es des {category}")
                    st.dataframe(df)
                    st.write("---")

                # Sauvegarder les donn√©es dans des fichiers CSV
                for category, df in all_data.items():
                    file_path = f"data/{category.lower().replace(' ', '_')}.csv"
                    df.to_csv(file_path, index=False)
                    st.success(f"Donn√©es sauvegard√©es pour {category} dans : `{file_path}`")

            else:
                data = scrap_data(URLS[selected_category], max_pages=num_pages)
                df = pd.DataFrame(data)
                if clean_data_option:
                    df = clean_data(df)
                st.markdown(f"### üìä Donn√©es des {selected_category}")
                st.dataframe(df)

                # Sauvegarder les donn√©es dans un fichier CSV
                file_path = f"data/{selected_category.lower().replace(' ', '_')}.csv"
                df.to_csv(file_path, index=False)
                st.success(f"Donn√©es sauvegard√©es dans : `{file_path}`")

            # Recharger les donn√©es apr√®s le scraping
            st.rerun()
# Titre du Dashboard
elif option == "Dashboard":
    st.header("üìà Dashboard des donn√©es scrap√©es")

    # S√©lection de la cat√©gorie
    selected_dashboard_category = st.selectbox("Choisir une cat√©gorie √† analyser :", list(URLS.keys()))

    file_path = f"data/webscraper_data/{selected_dashboard_category.lower().replace(' ', '_')}.csv"
    print(file_path)
    try:
        df_dashboard = pd.read_csv(file_path)

        clean_data(df_dashboard)
        # Nettoyage des donn√©es
       # df_dashboard["Prix"] = pd.to_numeric(df_dashboard["Prix"], errors="coerce").fillna(0)
        #df_dashboard["Localisation"] = df_dashboard["Localisation"].fillna("Non renseign√©")

        # Afficher le Dashboard
        display_dashboard(df_dashboard, selected_dashboard_category)

    except FileNotFoundError:
        st.warning(f"‚ö†Ô∏è Aucune donn√©e trouv√©e pour {selected_dashboard_category}. Veuillez scraper d'abord.")


# Option 3 : WebScraper Data
# Option 3 : WebScraper Data
elif option == "WebScraper Data":
    st.header("üåê Donn√©es WebScraper")
    st.write("T√©l√©chargez les fichiers de donn√©es scrap√©es disponibles dans `data/`.")

    # V√©rifier l'existence du dossier "data"
    data_folder = "data/webscraper_data"
    if not os.path.exists(data_folder):
        st.warning("‚ö†Ô∏è Le dossier 'data/' est introuvable. Veuillez d'abord scraper des donn√©es.")
    else:
        # Liste des fichiers disponibles dans le dossier 'data/'
        files = [f for f in os.listdir(data_folder) if f.endswith(".csv") or f.endswith(".json")]

        if files:
            for file in files:
                file_path = os.path.join(data_folder, file)

                # Lire le fichier pour le t√©l√©chargement
                with open(file_path, "rb") as f:
                    file_bytes = f.read()

                # Affichage avec ic√¥ne de t√©l√©chargement
                col1, col2 = st.columns([4, 1])
                with col1:
                    st.write(f"üìÑ {file}")
                with col2:
                    st.download_button(
                        label="‚¨áÔ∏è",
                        data=file_bytes,
                        file_name=file,
                        mime="text/csv" if file.endswith(".csv") else "application/json"
                    )
        else:
            st.warning("‚ö†Ô∏è Aucun fichier disponible dans `data/`. Veuillez d'abord scraper des donn√©es.")



# Option 4 : √âvaluer l'App
elif option == "√âvaluer l'App":
    st.header("√âvaluer l'application")
    st.write("Merci de remplir ce formulaire pour nous aider √† am√©liorer l'application.")

    # Lien du formulaire KoboToolbox
    kobo_form_url = "https://ee.kobotoolbox.org/x/bCaC927U"  # Remplace par ton propre lien

    # Utiliser iframe pour afficher le formulaire KoboToolbox
    iframe_code = f'<iframe src="{kobo_form_url}" width="100%" height="600px"></iframe>'
    st.markdown(iframe_code, unsafe_allow_html=True)

